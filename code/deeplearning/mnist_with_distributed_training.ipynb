{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "mnist_with_distributed_training.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYXhqJSiIlC3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Distributed Training for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "Install the following packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UN5aC7CYBnGR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f02e9260-7bb8-4e86-89ab-d3836d3465f2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "try:\n",
    "    from cloudmesh.common.StopWatch import StopWatch\n",
    "except:  # noqa: E722\n",
    "    ! pip install cloudmesh-common\n",
    "    from cloudmesh.common.StopWatch import StopWatch\n",
    "from cloudmesh.common.Shell import Shell    # noqa: E402"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting Output Graphs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def save(file):\n",
    "    if os.path.exists(\"images\"):\n",
    "        pass\n",
    "    else:\n",
    "        Shell.mkdir(\"images\")\n",
    "    plot_model(file, to_file=f'images/mnist-distributed training.png', show_shapes=True)\n",
    "    plot_model(file, to_file=f'images/mnist-distributed-training.pdf', show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qn-x5O_xBiit",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "StopWatch.start(\"total\")\n",
    "StopWatch.progress(0)\n",
    "StopWatch.start(\"import\")\n",
    "\n",
    "import numpy as np    # noqa: E402\n",
    "import tensorflow as tf    # noqa: E402\n",
    "from keras.models import Sequential    # noqa: E402\n",
    "from keras.layers import Dense, Activation, SimpleRNN, InputLayer, LSTM, Dropout    # noqa: E402\n",
    "from keras.utils import to_categorical, plot_model    # noqa: E402\n",
    "from keras.datasets import mnist    # noqa: E402\n",
    "from cloudmesh.common.StopWatch import StopWatch    # noqa: E402\n",
    "import os    # noqa: E402\n",
    "\n",
    "StopWatch.stop(\"import\")\n",
    "StopWatch.progress(1)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=0 pid=10248\n",
      "# cloudmesh status=running progress=1 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Load"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=2 pid=10248\n"
     ]
    }
   ],
   "source": [
    "StopWatch.start(\"data-load\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "StopWatch.stop(\"data-load\")\n",
    "StopWatch.progress(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pre-Process"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amkZr0dbvt7M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bb843a13-502a-49fa-b0eb-5b1d5f1668e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "StopWatch.start(\"data-pre-process\")\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "StopWatch.stop(\"data-pre-process\")\n",
    "StopWatch.progress(3)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=3 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku3FZI9VvvDt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNT4-APe-Jf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we use the Tensorflow distributed training components to train the model in multiple CPUs or GPUs. In the Colab instance multiple GPUs are not supported. Hence, the training must be done in the device type 'None' when selecting the 'runtime type' from Runtime menu. To run with multiple-GPUs no code change is required. [Learn more about distributed training](https://www.tensorflow.org/guide/distributed_training)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xr1SPagvxpq",
    "outputId": "1b026d05-8aa8-4ec0-ea43-7bd75fc16f14",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "StopWatch.start(\"compile\")\n",
    "\n",
    "input_shape = (image_size, image_size)\n",
    "batch_size = 128\n",
    "units = 256\n",
    "dropout = 0.2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "  model = Sequential()\n",
    "  # LSTM Layers\n",
    "  model.add(LSTM(units=units,                      \n",
    "                      input_shape=input_shape,\n",
    "                      return_sequences=True))\n",
    "  model.add(LSTM(units=units, \n",
    "                      dropout=dropout,                      \n",
    "                      return_sequences=True))\n",
    "  model.add(LSTM(units=units, \n",
    "                      dropout=dropout,                      \n",
    "                      return_sequences=False))\n",
    "\n",
    "  # MLP Layers\n",
    "  model.add(Dense(units))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(dropout))\n",
    "  model.add(Dense(units))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(dropout))\n",
    "  # Softmax_layer\n",
    "  model.add(Dense(num_labels))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.summary()\n",
    "  save(model)\n",
    "\n",
    "  print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "StopWatch.stop(\"compile\")\n",
    "StopWatch.progress(4)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 28, 256)           291840    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 28, 256)           525312    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,476,618\n",
      "Trainable params: 1,476,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of devices: 1\n",
      "# cloudmesh status=running progress=4 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hwr9YOzv4aY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe9Mskn7v5cv",
    "outputId": "cb6d03d1-cb96-450b-f2d8-e84b86474650",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "StopWatch.start(\"train\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=30, batch_size=batch_size)\n",
    "\n",
    "StopWatch.stop(\"train\")\n",
    "StopWatch.progress(99)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:Error reported to Coordinator: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\abeck\\ENV3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\abeck\\ENV3\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 225, in _call_for_each_replica\n",
      "    t.has_paused.wait()\n",
      "  File \"C:\\Users\\abeck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"C:\\Users\\abeck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPGR0hNJv6ln",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-htoQ51sv71v",
    "outputId": "e2b0da63-759c-4ed4-e692-da6a663f9b34",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "StopWatch.start(\"evaluate\")\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "\n",
    "StopWatch.stop(\"evaluate\")\n",
    "StopWatch.stop(\"total\")\n",
    "StopWatch.benchmark()\n",
    "StopWatch.progress(100)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8vqRCcsDEeu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "1. [Advance Deep Learning with Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)\n",
    "2. [Distributed With Tensorflow](https://www.tensorflow.org/guide/distributed_training)\n",
    "3. [Keras with Tensorflow Distributed Training](https://keras.io/guides/distributed_training/)"
   ]
  }
 ]
}