{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "mnist_with_distributed_training.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYXhqJSiIlC3"
   },
   "source": [
    "# Distributed Training for MNIST\n",
    "TODO: Test mnist_with_distributed_training on PyCharm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "Install the following packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UN5aC7CYBnGR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f02e9260-7bb8-4e86-89ab-d3836d3465f2"
   },
   "source": [
    "try:\n",
    "    from cloudmesh.common.StopWatch import StopWatch\n",
    "except:  # noqa: E722\n",
    "    ! pip install cloudmesh-common\n",
    "    from cloudmesh.common.StopWatch import StopWatch"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting Output Graphs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save(graph, filename):\n",
    "    if os.path.exists(\"images\"):\n",
    "        pass\n",
    "    else:\n",
    "        Shell.mkdir(\"images\")\n",
    "    plot_model(graph, to_file=f'images/{filename}.png', show_shapes=True)\n",
    "    plot_model(graph, to_file=f'images/{filename}.pdf', show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qn-x5O_xBiit"
   },
   "source": [
    "StopWatch.start(\"total\")\n",
    "StopWatch.start(\"import\")\n",
    "\n",
    "import os    # noqa: E402\n",
    "import cpuinfo    # noqa: E402\n",
    "import numpy as np    # noqa: E402\n",
    "import tensorflow as tf    # noqa: E402\n",
    "from keras.models import Sequential    # noqa: E402\n",
    "from keras.layers import Dense, Activation, SimpleRNN, InputLayer, LSTM, Dropout    # noqa: E402\n",
    "from keras.utils import to_categorical, plot_model    # noqa: E402\n",
    "from keras.datasets import mnist    # noqa: E402\n",
    "from cloudmesh.common.systeminfo import os_is_windows    # noqa: E402\n",
    "from cloudmesh.common.Shell import Shell    # noqa: E402\n",
    "\n",
    "filename = Shell.map_filename(f'~/reu2022/code/deeplearning/mnist/mnist_with_distributed_training.log').path\n",
    "StopWatch.progress(0, filename=filename)\n",
    "\n",
    "StopWatch.stop(\"import\")\n",
    "StopWatch.progress(1, filename=filename)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=0 pid=10248\n",
      "# cloudmesh status=running progress=1 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Load"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=2 pid=10248\n"
     ]
    }
   ],
   "source": [
    "StopWatch.start(\"data-load\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "StopWatch.stop(\"data-load\")\n",
    "StopWatch.progress(2, filename=filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pre-Process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amkZr0dbvt7M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bb843a13-502a-49fa-b0eb-5b1d5f1668e6"
   },
   "source": [
    "StopWatch.start(\"data-pre-process\")\n",
    "\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size])\n",
    "x_test = np.reshape(x_test,[-1, image_size, image_size])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "StopWatch.stop(\"data-pre-process\")\n",
    "StopWatch.progress(3, filename=filename)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cloudmesh status=running progress=3 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku3FZI9VvvDt"
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNT4-APe-Jf"
   },
   "source": [
    "Here we use the Tensorflow distributed training components to train the model in multiple CPUs or GPUs. In the Colab instance multiple GPUs are not supported. Hence, the training must be done in the device type 'None' when selecting the 'runtime type' from Runtime menu. To run with multiple-GPUs no code change is required. [Learn more about distributed training](https://www.tensorflow.org/guide/distributed_training)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xr1SPagvxpq",
    "outputId": "1b026d05-8aa8-4ec0-ea43-7bd75fc16f14"
   },
   "source": [
    "StopWatch.start(\"compile\")\n",
    "\n",
    "input_shape = (image_size, image_size)\n",
    "batch_size = 128\n",
    "units = 256\n",
    "dropout = 0.2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "  model = Sequential()\n",
    "  # LSTM Layers\n",
    "  model.add(LSTM(units=units,                      \n",
    "                      input_shape=input_shape,\n",
    "                      return_sequences=True))\n",
    "  model.add(LSTM(units=units, \n",
    "                      dropout=dropout,                      \n",
    "                      return_sequences=True))\n",
    "  model.add(LSTM(units=units, \n",
    "                      dropout=dropout,                      \n",
    "                      return_sequences=False))\n",
    "\n",
    "  # MLP Layers\n",
    "  model.add(Dense(units))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(dropout))\n",
    "  model.add(Dense(units))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(dropout))\n",
    "\n",
    "  # Softmax_layer\n",
    "  model.add(Dense(num_labels))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.summary()\n",
    "  save(model, 'mnist_with_distributed_training')\n",
    "\n",
    "  print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "StopWatch.stop(\"compile\")\n",
    "StopWatch.progress(4, filename=filename)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 28, 256)           291840    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 28, 256)           525312    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,476,618\n",
      "Trainable params: 1,476,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Number of devices: 1\n",
      "# cloudmesh status=running progress=4 pid=10248\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hwr9YOzv4aY"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe9Mskn7v5cv",
    "outputId": "cb6d03d1-cb96-450b-f2d8-e84b86474650"
   },
   "source": [
    "StopWatch.start(\"train\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=batch_size)\n",
    "\n",
    "StopWatch.stop(\"train\")\n",
    "StopWatch.progress(99, filename=filename)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "INFO:tensorflow:Error reported to Coordinator: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\abeck\\ENV3\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 293, in stop_on_exception\n",
      "    yield\n",
      "  File \"C:\\Users\\abeck\\ENV3\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 225, in _call_for_each_replica\n",
      "    t.has_paused.wait()\n",
      "  File \"C:\\Users\\abeck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"C:\\Users\\abeck\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPGR0hNJv6ln"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-htoQ51sv71v",
    "outputId": "e2b0da63-759c-4ed4-e692-da6a663f9b34"
   },
   "source": [
    "StopWatch.start(\"test\")\n",
    "\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n",
    "\n",
    "StopWatch.stop(\"test\")\n",
    "StopWatch.stop(\"total\")\n",
    "StopWatch.progress(100, filename=filename)\n",
    "\n",
    "if os_is_windows():\n",
    "    user = os.environ[\"USERNAME\"]\n",
    "else:\n",
    "    try:\n",
    "        user = os.environ['USER']\n",
    "    except:  # noqa: E722\n",
    "        user = os.system('basename $HOME')\n",
    "\n",
    "try:\n",
    "    gpuname = ''\n",
    "    for line in open('mlp_mnist.log', 'r'):\n",
    "        if 'GPU' in line and line[-2] == ')':\n",
    "            gpuname = gpuname + line[:line.find('(')] + '\\n'\n",
    "except:  # noqa: E722\n",
    "    gpuname = cpuinfo.get_cpu_info()['brand_raw']\n",
    "\n",
    "tag = 'mlp_mnist'\n",
    "\n",
    "StopWatch.benchmark(tag=tag, node=gpuname, user=user)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8vqRCcsDEeu"
   },
   "source": [
    "# References\n",
    "\n",
    "1. [Advance Deep Learning with Keras](https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras)\n",
    "2. [Distributed With Tensorflow](https://www.tensorflow.org/guide/distributed_training)\n",
    "3. [Keras with Tensorflow Distributed Training](https://keras.io/guides/distributed_training/)"
   ]
  }
 ]
}
